<h2 id="intro" class="list">Introduction <span class="backlink"> back to <a href="#toc">ToC</a></span></h2>
<span class="markdown">
The explainability is necessary but far from sufficient for achieving the desired trustworthiness in AI systems. In order to do so, not only should the developed AI systems be explainable, but also accountable. As a matter of fact, the ability to hold them accountable by explaining their inner workings, their results and the causes of failures to users, regulators and citizens, is critical to achieve trust.

The accountability can be defined as the ability to determine whether a decision was made in accordance with procedural and substantive standards and to hold someone responsible if those standards are not met. This means that with an accountable AI system, the causes that derived a given decision can be discovered, even if its underlying model's details are not fully known or must be kept secret. In other words, the person, group or company in charge of the AI system should be able to answer questions that are related, not only to the obtained outputs (e.g. what the output result is or when the output is generated), but also to the AI procedures that led to such outputs (e.g. which data set(s) are used to train the AI system or how well the AI system performs in terms of accuracy).

However, the information needed to answer these questions is hardly ever accessible in a straightforward way. This information is scattered across multiple files, repositories and systems, and in the worst-case scenario, is not even registered. That means that, if the person, group or company in charge of the AI system wanted to answer the aforementioned questions, it would be very time consuming, as it would be needed to be an expert or have the help of experts in different frameworks, systems, data models, repositories and query languages. As a matter of fact, the regular performance these accountancy tasks would be infeasible.

Therefore, it seems reasonable to consider that the adequate representation of data, processes and workflows involved in AI systems could contribute to make them accountable in an easier and systematic manner. There are a variety of technologies that offer conceptual modelling capabilities to describe a domain of interest, but only ontologies combine this feature with Web compliance, formality and reasoning capabilities</span>
<div id="namespacedeclarations">
<h3 id="ns" class="list">Namespace declarations</h3>
<div id="ns" align="center">
<table>
<caption> <a href="#ns"> Table 1</a>: Namespaces used in the document </caption>
<tbody>
<tr><td><b>w3id-org</b></td><td>&lt;https://w3id.org/&gt;</td></tr>
<tr><td><b>ns</b></td><td>&lt;http://www.w3.org/2003/06/sw-vocab-status/ns#&gt;</td></tr>
<tr><td><b>owl</b></td><td>&lt;http://www.w3.org/2002/07/owl#&gt;</td></tr>
<tr><td><b>cpannotationschema</b></td><td>&lt;http://www.ontologydesignpatterns.org/schemas/cpannotationschema.owl#&gt;</td></tr>
<tr><td><b>xsd</b></td><td>&lt;http://www.w3.org/2001/XMLSchema#&gt;</td></tr>
<tr><td><b>voaf</b></td><td>&lt;http://purl.org/vocommons/voaf#&gt;</td></tr>
<tr><td><b>rdfs</b></td><td>&lt;http://www.w3.org/2000/01/rdf-schema#&gt;</td></tr>
<tr><td><b>ns1</b></td><td>&lt;http://creativecommons.org/ns#&gt;</td></tr>
<tr><td><b>eep</b></td><td>&lt;https://w3id.org/eep#&gt;</td></tr>
<tr><td><b>mls</b></td><td>&lt;http://www.w3.org/ns/mls#&gt;</td></tr>
<tr><td><b>rdf</b></td><td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;</td></tr>
<tr><td><b>terms</b></td><td>&lt;http://purl.org/dc/terms/&gt;</td></tr>
<tr><td><b>bibo</b></td><td>&lt;http://purl.org/ontology/bibo/&gt;</td></tr>
<tr><td><b>vann</b></td><td>&lt;http://purl.org/vocab/vann/&gt;</td></tr>
<tr><td><b>fides</b></td><td>&lt;https://w3id.org/fides#&gt;</td></tr>
<tr><td><b>dc</b></td><td>&lt;http://purl.org/dc/elements/1.1/&gt;</td></tr>
</tbody>
</table>
</div>
</div>
